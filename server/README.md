#ModelServer:

##Introduction

This document provides guidance on setting up and running the ModelServer, a FastAPI application designed to utilize LLMs (specifically "microsoft/phi-2" or "google/gemma-7b") to generate criticisms for essays or letters generated by other LLMs. It uses HuggingFace's transformers library for ML model interfacing and LangChain for structured responses.

###Features

    Model Evaluation: Evaluates essays based on predefined prompts and generates critical feedback.
    Dockerized Application: Ensures consistent environment and easy deployment.
    Configurable Model Selection: Allows users to specify the model via environment variables.

###Prerequisites

Before you start, ensure you have the following installed:

    1) Docker
    2) Docker Compose
    3) Optionally, for local testing without Docker:
       Python (version 3.8 or later recommended)
       Required Python libraries (FastAPI, uvicorn, transformers, etc.) which can be installed using pip (mentioned in the requirments.txt file)
       Ensure port 8000 is free for the fastapi server

##Setup and Installation

###Clone the Repository

To get started, clone this repository to your local machine:

`
git clone https://github.com/abhimazu/binoloop.git
cd binoloop/server
`

###Build and Run with Docker

To build and run the server using Docker Compose, execute the following commands from the root of your project directory:

`
chmod +x run.sh
./run.sh
'

This command will build the Docker image if it hasn't been built and start the server using docker-compose file. The server will be accessible at http://localhost:8000.

###Environment Variables

The application uses the following environment variables, which can be set in the docker-compose.yml file:

    MODEL_NAME: The name of the Hugging Face model to use (either "microsoft/phi-2" or "google/gemma-7b")
    NUM_WORKERS: The number of worker processes for handling requests for the uvicorn server.

##Run Locally Without Docker (Optional)

If you prefer to run the application locally without Docker, ensure you set up a virtual environment, install the dependencies, and run:

'
chmod +x script_run.sh
./script_run.sh
'

###Usage

Once the server is running, you can evaluate essays by sending HTTP POST requests to http://localhost:8000/evaluate. The example of how send a request is in the [client](https://github.com/abhimazu/binoloop/tree/main/client) folder:


###Logs

Logs are generated by the application and by default, are printed to stdout and saved inside the logs folder that is created after running the .sh file. In a Docker environment, these logs can be viewed by running:

'
docker-compose logs
'

