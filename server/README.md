# ModelServer:

## Introduction

This document provides guidance on setting up and running the ModelServer, a FastAPI application designed to utilize LLMs (specifically "microsoft/phi-2" or "google/gemma-7b") to generate criticisms for essays or letters generated by other LLMs. It uses HuggingFace's transformers library for ML model interfacing and LangChain for structured responses.

### Features

1. **Model Evaluation**: Evaluates essays based on predefined prompts and generates critical feedback.
2. **Dockerized Application**: Ensures consistent environment and easy deployment.
3. **Configurable Model Selection**: Allows users to specify the model via environment variables.

### Prerequisites

Before you start, ensure you have the following installed:

1. Docker
2. Docker Compose
3. Optionally, for local testing without Docker:
   - Python (version 3.8 or later recommended)
   - Required Python libraries (FastAPI, uvicorn, transformers, etc.) which can be installed using pip (mentioned in the requirments.txt file)
   - Ensure port 8000 is free for the fastapi server

## Setup and Installation

### Clone the Repository

To get started, clone this repository to your local machine:

```
git clone https://github.com/abhimazu/binoloop.git
cd binoloop/server
```

### Build and Run with Docker

To build and run the server using Docker Compose, execute the following commands from the root of your project directory:

```
chmod +x run.sh
./run.sh
```

This command will build the Docker image if it hasn't been built and start the server using docker-compose file. The server will be accessible at http://localhost:8000.

### Environment Variables

The application uses the following environment variables, which can be set in the docker-compose.yml file:

    MODEL_NAME: The name of the Hugging Face model to use (either "microsoft/phi-2" or "google/gemma-7b")
    NUM_WORKERS: The number of worker processes for handling requests for the uvicorn server.

## Run Locally Without Docker (Optional)

If you prefer to run the application locally without Docker, ensure you set up a virtual environment, install the dependencies, and run:

```
chmod +x script_run.sh
./script_run.sh
```
## How it works

### Huggingface pipeline definition

A huggingface transformers based pipeline is defined for the model with hyperparams:

```
tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map='auto',
    torch_dtype=torch.bfloat16
)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=1024,
    temperature=0.4
)

local_llm = HuggingFacePipeline(pipeline=pipe)
```

Then, the template prompt for evaluation and the LLM chain is defined: (Yes, you can play with the template to check how it affects the response!)

```
template = """You a response evaluator. You are tasked with generating criticisms for the relevance of an essay or letter written below as per the question asked:
### Question:
{prompt}

### Answer:
{instruction}

Evaluation:"""

prompt = PromptTemplate(template=template, input_variables=["prompt", "instruction"])
llm_chain = LLMChain(prompt=prompt, llm=local_llm)
```

The schema for requests and responses are defined for standardization. This ensures that they include all parameters that are important for the application.

```
class EssayEvaluationRequest(BaseModel):
    prompt_id: int
    essay_output: str
    student_id: str

class EssayEvaluationResponse(BaseModel):
    criticism: str
    student_id: str
    prompt_id: int
```
Finally, a Fast API post function with **"/evaluate"** end point is defined that invokes the chain whenever a request is received:

```
@app.post("/evaluate", response_model=EssayEvaluationResponse)
async def evaluate_essay(request: EssayEvaluationRequest):
    try:
        input_dict = {
            "prompt": prompt_dict[request.prompt_id],  
            "instruction": request.essay_output  
        }
        result = llm_chain.invoke(input_dict)  
        return EssayEvaluationResponse(criticism=result, student_id=request.student_id, prompt_id=request.prompt_id)

``` 

### Usage

Once the server is running, you can evaluate essays by sending HTTP POST requests to http://localhost:8000/evaluate. The example of how send a request is in the [client](https://github.com/abhimazu/binoloop/tree/main/client) folder:


### Logs

Logs are generated by the application and by default, are printed to stdout and saved inside the logs folder that is created after running the .sh file. In a Docker environment, these logs can be viewed by running:

```
docker-compose logs
```

