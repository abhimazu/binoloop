# Request Server:

## Introduction

This document provides guidance on setting up and running the Server, a FastAPI application designed to utilize Trition LLM model server (specifically "microsoft/phi-2" or "google/gemma-7b") to generate criticisms for essays or letters generated by other LLMs. It aggregates requests and forwards the requests to a Trition server based on Model name and sends back the responses.

### Features

1. **Model Evaluation**: Evaluates essays based on predefined prompts and generates critical feedback.
2. **Dockerized Application**: Ensures consistent environment and easy deployment.
3. **Configurable Model Selection**: Allows users to specify the model via environment variables.

### Prerequisites

Before you start, ensure you have the following installed:

1. Docker
2. Docker Compose
3. Optionally, for local testing without Docker:
   - Python (version 3.8 or later recommended)
   - Required Python libraries (FastAPI, uvicorn, transformers, etc.) which can be installed using pip (mentioned in the requirments.txt file)

4. Ensure port 8000 is free for the fastapi server
5. Ensure the Trition server is up and running
   
## Setup and Installation

### Clone the Repository

To get started, clone this repository to your local machine:

```
git clone https://github.com/abhimazu/binoloop.git
cd binoloop/server
```

### Build and Run with Docker

To build and run the server using Docker Compose, execute the following commands from the root of your project directory:

```
chmod +x run.sh
./run.sh
```

This command will build the Docker image if it hasn't been built and start the server using docker-compose file. The server will be accessible at http://localhost:8000.

## Run Locally Without Docker (Optional)

If you prefer to run the application locally without Docker, ensure you set up a virtual environment, install the dependencies, and run:

```
chmod +x script_run.sh
./script_run.sh
```

