# Model Client

## Introduction

This document contains the necessary information to set up and run the Model Client, a Python script designed to send requests to the Model Server created from the [server](https://github.com/abhimazu/binoloop/tree/main/server) for evaluating essays generated by LLMs. The client reads data from a CSV file that contains essay generation entries, parses the file, sends requests to the server for evaluation, and stores the responses.

### Features

1. **Automated Requests**: Automatically sends requests to the ModelServer.
2. **Configurable Parameters**: Allows customization of server address, data file, and operational parameters through command-line arguments.
3. **Logging**: Logs all operations, providing a clear trace of activities and errors.

### Prerequisites

Before running this script, ensure you have the following:

- Python (version 3.7 or later recommended)
- [Dataset](https://www.kaggle.com/datasets/dardodel/4k-mixtral87b-crafted-essays-for-detect-ai-comp) downloaded from kaggle and stored in clients directory.

## Setup and Installation

Clone the Repository

```
git clone https://github.com/abhimazu/binoloop.git
cd binoloop/client
```

### Configuration

You can configure the client using command-line arguments. Below are the available options:

- --host: Specifies the host IP address of the ModelServer (default: localhost).
- --port: Specifies the port on which the ModelServer is running (default: 8000).
- --file: Specifies the CSV file containing the essay data (default: Mixtral8x7b_4k_essays_for_DetectAIGeneratedTextCompetition.csv).
- --start: Specifies the start index in the CSV file to begin processing (default: 0).
- --end: Specifies the end index in the CSV file to stop processing (default: 10).
- --interval: Specifies the interval in seconds between consecutive server requests (default: 2).

## Running the Client

To run the client script, use the following command:

```
chmod +x run.sh
./run.sh
```

This command creates a python virtual environment in cthe client folder, installs all requirements and runs the clinet.py file. The script sends requests to the ModelServer running at **host:port**, processing essay data from the **csv** for entries **start** to **end**, with an **interval** between requests.

## How it works

The send request funtion sends a single essay to the server that is running at **host:port** */evaluate* and returns the response back:

```
def send_request(row):
    essay_data = {
        "prompt_id": row['prompt_id'],
        "essay_output": row["AI_Essay"], 
        "student_id": row["student_id"]
    }
    try:
        response = requests.post(f"{BASE_URL}/evaluate", json=essay_data, timeout=3000)
        response.raise_for_status()
        return response.json()
```

Another function processes the dataset with the range specified and calls the function from above by passing row-wise arguments to the function. Here the df is the subset of the dataset specified by range start to end. This function returns a list of resonses for the entire dataset range.  

```
def process_dataset_loop(df, sleep_time):
    results = []
    for index, row in df.iterrows():
        results.append(send_request(row))
        time.sleep(sleep_time) 
    return results

df = pd.read_csv(args.file)
ranged_df = df.iloc[args.start:args.end]

responses = process_dataset_loop(ranged_df, args.interval)
```

### Output

The client saves the server responses to a CSV file named *evaluation_responses.csv* in the current working directory. 

### Logs

Logs are generated and stored in the logs/ directory. These logs include detailed information about each request and response, errors encountered, and other significant events.

